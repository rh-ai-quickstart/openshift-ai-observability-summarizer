# DEV_GUIDE.md - OpenShift AI Observability Summarizer

> **Comprehensive Development Guide for Human Developers & AI Assistants**  
> This file provides complete guidance for working with the AI Observability Summarizer project, combining development patterns, architecture, and comprehensive instructions for both **human developers** and **AI coding assistants**.

## ğŸš€ Project Overview

The **OpenShift AI Observability Summarizer** is an open source, CNCF-style project that provides advanced monitoring and automated summarization of AI model and OpenShift cluster metrics. It generates AI-powered insights and reports from Prometheus/Thanos metrics data.

### Key Capabilities
- **vLLM Monitoring**: GPU usage, latency, request volume analysis
- **OpenShift Fleet Monitoring**: Cluster-wide and namespace-scoped metrics
- **AI-Powered Insights**: LLM-based metric summarization and analysis
- **Report Generation**: HTML, PDF, and Markdown exports
- **Alerting & Notifications**: AI-powered alerts with Slack integration
- **Distributed Tracing**: OpenTelemetry and Tempo integration

## ğŸ“ Project Structure

```
summarizer/
â”œâ”€â”€ src/                    # Main source code
â”‚   â”œâ”€â”€ api/               # FastAPI metrics API
â”‚   â”‚   â”œâ”€â”€ metrics_api.py # Main API endpoints
â”‚   â”‚   â””â”€â”€ report_assets/ # Report generation assets
â”‚   â”œâ”€â”€ core/              # Core business logic
â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration management
â”‚   â”‚   â”œâ”€â”€ llm_client.py  # LLM communication
â”‚   â”‚   â”œâ”€â”€ metrics.py     # Metrics discovery & fetching
â”‚   â”‚   â”œâ”€â”€ analysis.py    # Statistical analysis
â”‚   â”‚   â”œâ”€â”€ reports.py     # Report generation
â”‚   â”‚   â”œâ”€â”€ promql_service.py # PromQL generation
â”‚   â”‚   â””â”€â”€ thanos_service.py # Thanos integration
â”‚   â”œâ”€â”€ ui/                # Streamlit UI
â”‚   â”‚   â””â”€â”€ ui.py         # Multi-dashboard interface
â”‚   â””â”€â”€ alerting/          # Alerting service
â”‚       â””â”€â”€ alert_receiver.py # Alert handling
â”œâ”€â”€ deploy/helm/           # Helm charts for deployment
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ scripts/               # Development and deployment scripts
â””â”€â”€ docs/                  # Documentation
```

## ğŸ”§ Development Setup

### Prerequisites
- Python 3.11+
- `uv` package manager
- OpenShift CLI (`oc`)
- `helm` v3.x
- `yq` (YAML processor)
- Docker or Podman

### Environment Setup
```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Sync dependencies
uv sync --group dev

# Activate virtual environment
source .venv/bin/activate

# Set namespace for development
export LLM_NAMESPACE=<your-namespace>
```

### Local Development
```bash
# Set up port-forwarding to cluster services
make deploy-local

# This runs ./scripts/local-dev.sh which sets up:
# - Port-forwarding to Llamastack (8000:8000)
# - Port-forwarding to llm-service (8001:8001)  
# - Port-forwarding to Thanos (9090:9090)
```

## ğŸ—ï¸ Architecture & Data Flow

### Core Components
1. **Metrics API** (`src/api/metrics_api.py`): FastAPI backend serving metrics analysis and chat endpoints
2. **UI** (`src/ui/ui.py`): Streamlit multi-dashboard frontend
3. **Core Logic** (`src/core/`): Business logic modules for metrics processing and LLM integration
4. **Alerting** (`src/alerting/`): Alert handling and Slack notifications
5. **Helm Charts** (`deploy/helm/`): OpenShift deployment configuration

### Data Flow
1. **Natural Language Question** â†’ PromQL generation via LLM
2. **PromQL Queries** â†’ Thanos/Prometheus for metrics data
3. **Metrics Data** â†’ Statistical analysis and anomaly detection
4. **Analysis Results** â†’ LLM summarization
5. **Summary** â†’ Report generation (HTML/PDF/Markdown)

### Key Services Integration
- **Prometheus/Thanos**: Metrics storage and querying
- **vLLM**: Model serving with /metrics endpoint
- **DCGM**: GPU monitoring metrics
- **Llama Stack**: LLM inference backend
- **OpenTelemetry/Tempo**: Distributed tracing

## ğŸ§ª Testing

### Test Commands
```bash
# Run all tests with coverage
uv run pytest -v --cov=src --cov-report=html --cov-report=term

# Run specific test categories
uv run pytest -v tests/mcp/           # MCP tests
uv run pytest -v tests/core/          # Core logic tests
uv run pytest -v tests/alerting/      # Alerting tests
uv run pytest -v tests/api/           # API tests

# Run specific test file
uv run pytest -v tests/mcp/test_api_endpoints.py

# View coverage report
open htmlcov/index.html
```

### Test Structure
- **`tests/mcp/`** - Metric Collection & Processing tests
- **`tests/core/`** - Core business logic tests
- **`tests/alerting/`** - Alerting service tests
- **`tests/api/`** - API endpoint tests

### Testing Strategy
- **Unit Tests**: Core business logic in `tests/core/`
- **Integration Tests**: API endpoints in `tests/mcp/`
- **Alert Tests**: Alerting functionality in `tests/alerting/`
- **Coverage**: Configured to exclude UI components and report assets

## ğŸš€ Building & Deployment

### Container Images
```bash
# Build all components
make build

# Build individual components
make build-metrics-api    # FastAPI backend
make build-ui            # Streamlit UI
make build-alerting      # Alerting service

# Build with custom tag
make build TAG=v1.0.0
```

### Deployment
```bash
# Deploy to OpenShift
make install NAMESPACE=your-namespace

# Deploy with alerting
make install-with-alerts NAMESPACE=your-namespace

# Deploy with specific LLM model
make install NAMESPACE=your-namespace LLM=llama-3-2-3b-instruct

# Deploy with GPU tolerations
make install NAMESPACE=your-namespace \
  LLM=llama-3-2-3b-instruct \
  LLM_TOLERATION="nvidia.com/gpu"

# Deploy with safety models
make install NAMESPACE=your-namespace \
  LLM=llama-3-2-3b-instruct \
  SAFETY=llama-guard-3-8b
```

### Management
```bash
# Check deployment status
make status NAMESPACE=your-namespace

# Uninstall
make uninstall NAMESPACE=your-namespace

# List available models
make list-models
```

## âš™ï¸ Configuration

### Environment Variables
- `PROMETHEUS_URL`: Thanos/Prometheus endpoint (default: http://localhost:9090)
- `LLAMA_STACK_URL`: LLM backend URL (default: http://localhost:8321/v1/openai/v1)
- `LLM_API_TOKEN`: API token for LLM service
- `MODEL_CONFIG`: JSON configuration for available models
- `THANOS_TOKEN`: Authentication token (default: reads from service account)
- `SLACK_WEBHOOK_URL`: Slack webhook for alerting notifications

### Model Configuration
Models are configured via `MODEL_CONFIG` environment variable as JSON:
```json
{
  "model-name": {
    "external": false,
    "url": "http://service:port",
    "apiToken": "token"
  }
}
```

### Available Models
```bash
# List available models for deployment
make list-models
```
Common models include:
- `llama-3-2-3b-instruct` (default)
- `llama-3-1-8b-instruct`
- `llama-3-3-70b-instruct`
- `llama-guard-3-8b` (safety model)

## ğŸ” Common Development Patterns

### Adding New Metrics
1. Update metric discovery functions in `src/core/metrics.py`
2. Add PromQL queries for the new metrics
3. Update UI components to display the metrics
4. Add corresponding tests

### Adding New LLM Endpoints
1. Define request/response models in `src/core/models.py`
2. Implement business logic in appropriate `src/core/` module
3. Add FastAPI endpoint in `src/api/metrics_api.py`
4. Add corresponding tests

### Error Handling
- API endpoints use HTTPException for user-facing errors
- Internal errors are logged with stack traces
- LLM API key errors return specific user-friendly messages

## ğŸš€ Development Workflows

### 1. Feature Development
```bash
# 1. Set up local environment
uv sync --group dev
source .venv/bin/activate

# 2. Set up port-forwarding
make deploy-local

# 3. Make changes to source code

# 4. Run tests
uv run pytest -v

# 5. Build and test locally
make build
```

### 2. Bug Fixing
```bash
# 1. Reproduce issue locally
make deploy-local

# 2. Run specific test
uv run pytest -v tests/core/test_specific_feature.py

# 3. Debug with coverage
uv run pytest -v --cov=src --cov-report=term-missing
```

### 3. Deployment Testing
```bash
# 1. Build images
make build TAG=test-$(date +%s)

# 2. Deploy to test namespace
make install NAMESPACE=test-namespace

# 3. Verify deployment
make status NAMESPACE=test-namespace

# 4. Test functionality
# Access UI via OpenShift route

# 5. Cleanup
make uninstall NAMESPACE=test-namespace
```

## ğŸ“Š Monitoring & Debugging

### Port Forwarding
```bash
# Manual port-forwarding (if make deploy-local fails)
oc port-forward svc/llama-stack 8000:8000 -n $LLM_NAMESPACE &
oc port-forward svc/llm-service 8001:8001 -n $LLM_NAMESPACE &
oc port-forward svc/thanos-query 9090:9090 -n $LLM_NAMESPACE &
```

### Logs
```bash
# View pod logs
oc logs -f deployment/metrics-api -n $LLM_NAMESPACE
oc logs -f deployment/metric-ui -n $LLM_NAMESPACE
oc logs -f deployment/metric-alerting -n $LLM_NAMESPACE
```

### Metrics
```bash
# Access Prometheus metrics
oc port-forward svc/metrics-api 8000:8000 -n $LLM_NAMESPACE
# Then visit http://localhost:8000/metrics
```

## ğŸ› ï¸ Useful Makefile Targets

### Development
- `make deploy-local` - Set up local development environment
- `make test` - Run unit tests with coverage
- `make clean` - Clean up local images

### Building
- `make build` - Build all container images
- `make build-metrics-api` - Build FastAPI backend
- `make build-ui` - Build Streamlit UI
- `make build-alerting` - Build alerting service

### Deployment
- `make install` - Deploy to OpenShift
- `make install-with-alerts` - Deploy with alerting
- `make status` - Check deployment status
- `make uninstall` - Remove deployment

### Configuration
- `make config` - Show current configuration
- `make list-models` - List available LLM models
- `make help` - Show all available targets

## ğŸ”§ Troubleshooting

### Common Issues

#### Port Forwarding Fails
```bash
# Check if pods are running
oc get pods -n $LLM_NAMESPACE

# Restart port-forwarding
make deploy-local
```

#### Tests Fail
```bash
# Ensure virtual environment is activated
source .venv/bin/activate

# Reinstall dependencies
uv sync --group dev --reinstall

# Run tests with verbose output
uv run pytest -v --tb=short
```

#### Build Fails
```bash
# Check Docker/Podman is running
docker ps

# Clean and rebuild
make clean
make build
```

#### Deployment Issues
```bash
# Check namespace exists
oc get namespace $LLM_NAMESPACE

# Check Helm releases
helm list -n $LLM_NAMESPACE

# View deployment events
oc get events -n $LLM_NAMESPACE --sort-by='.lastTimestamp'
```

## ğŸ”’ Security Considerations
- Service account tokens are read from mounted volumes
- SSL verification uses cluster CA bundle when available
- No secrets should be logged or committed to repository
- API endpoints use proper authentication and authorization

## ğŸ“š Additional Resources

- **README.md** - Comprehensive project overview and setup
- **docs/GITHUB_ACTIONS.md** - CI/CD workflow documentation
- **docs/SEMANTIC_VERSIONING.md** - Version management guidelines

## ğŸ¯ Quick Reference

### File Locations
- **Main API**: `src/api/metrics_api.py`
- **Core Logic**: `src/core/llm_summary_service.py`
- **UI**: `src/ui/ui.py`
- **Tests**: `tests/`
- **Helm Charts**: `deploy/helm/`

### Key Commands
- **Local Dev**: `make deploy-local`
- **Tests**: `uv run pytest -v`
- **Build**: `make build`
- **Deploy**: `make install NAMESPACE=ns`
- **Status**: `make status NAMESPACE=ns`

### Environment Variables
- `LLM_NAMESPACE` - Target OpenShift namespace
- `REGISTRY` - Container registry (default: quay.io)
- `VERSION` - Image version (default: 0.1.2)
- `LLM` - LLM model ID for deployment
- `PROMETHEUS_URL` - Metrics endpoint
- `LLAMA_STACK_URL` - LLM backend URL

---

**ğŸ’¡ Tip**: Use `make help` to see all available Makefile targets and their descriptions.
